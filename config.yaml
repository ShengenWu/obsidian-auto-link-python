# Obsidian Auto-Link Core 配置文件

# 你的 Obsidian 仓库根目录
vault_path: "./my_test_vault"

# ---------------------------------------------------------
# LLM 服务商配置 (多账号管理)
# ---------------------------------------------------------
# 当前Tagging功能和Linking功能使用的服务商名称 (必须对应下方 providers 中的 key)
active_provider: "aihubmix-router"

providers:
  # 示例 1: DeepSeek (OpenAI 兼容协议)
  aihubmix-router:
    provider_type: "openai_compatible" # 对应 OpenAI 客户端
    base_url: "https://aihubmix.com/v1"
    api_key: "${AIHUBMIX_API_KEY}" # 支持从环境变量读取
    model: "aihubmix-router"
    temperature: 0.3

  local-gemini3-flash-api:
    provider_type: "openai_compatible" # 对应 OpenAI 客户端
    base_url: "http://127.0.0.1:8045/v1"
    api_key: "${LOCAL_API_KEY}" # 支持从环境变量读取
    model: "gemini-3-flash"
    temperature: 0.3

  local-gemini3-pro-api:
    provider_type: "openai_compatible" # 对应 OpenAI 客户端
    base_url: "http://127.0.0.1:8045/v1"
    api_key: "${LOCAL_API_KEY}" # 支持从环境变量读取
    model: "gemini-3-pro-low"
    temperature: 0.3

  # 示例 2: OpenAI 官方
  openai-official:
    provider_type: "openai"
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4o"
    temperature: 0.5

  # 示例 3: Anthropic (Claude)
  anthropic-cloud:
    provider_type: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"
    model: "claude-3-5-sonnet-latest"
    temperature: 0.3

  # 示例 4: Google Gemini
  google-gemini:
    provider_type: "google"
    api_key: "${GOOGLE_API_KEY}"
    model: "gemini-1.5-pro"
    temperature: 0.3

# ---------------------------------------------------------
# Prompt 管理
# ---------------------------------------------------------
# Prompt 模板文件路径
prompt_file: "prompts.yaml"

# ---------------------------------------------------------
# 向量化配置
# ---------------------------------------------------------
embedding:
  type: "local" # "local" or "api"
  model_name: "BAAI/bge-large-zh-v1.5"

# ---------------------------------------------------------
# 摘要策略配置 (Summarization)
# ---------------------------------------------------------
summarization:
  enable: true
  # 独立指定摘要用的模型服务商 (必须在 providers 中定义)
  # 如果留空，则默认使用 active_provider (主模型)
  # 建议：使用便宜、快速的模型 (如 gemini-flash, gpt-3.5-turbo)
  provider: "aihubmix-router"
  # 超过多少字符触发处理 (摘要或截断)
  threshold: 1000
  # 开启摘要时：喂给摘要模型的最大字符数 (建议设大一点，如 6000)
  max_input_length: 6000
  # 关闭摘要时：直接截断的长度 (建议设小一点，如 2000，节省主模型 Token)
  hard_truncate_length: 2000

# ---------------------------------------------------------
# 流程与安全
# ---------------------------------------------------------
pipeline:
  dry_run: false # 模拟模式，用于测试，所有更改不会被写入文件，而是打印在控制台上

safety:
  enable_backup: true # 修改前是否备份文件 (强烈建议开启)
  backup_retention_days: 7
  backup_path: "./.auto_link_backups"

reporting:
  enable_summary: true
  log_folder: "System/Auto-Link-Logs"
